{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q --force-reinstall requests\n",
    "!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite]\"\n",
    "!python -c \"import aim\" || pip install -q aim\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNETR\n",
    "\n",
    "from monai.data import (\n",
    "    DataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    ")\n",
    "\n",
    "\n",
    "from monai.losses import DiceCELoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    Resize,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    RandCropByPosNegLabeld,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  Lambdad\n",
    ")\n",
    "from monai.data import CacheDataset, Dataset\n",
    "import torch\n",
    "\n",
    "patch_size = 16\n",
    "spatial_size = (64, 64, 96)\n",
    "\n",
    "adjusted_spatial_size = tuple(((dim + patch_size - 1) // patch_size) * patch_size for dim in spatial_size)\n",
    "\n",
    "def debug_transform(data):\n",
    "    try:\n",
    "        for transform in val_transforms.transforms:\n",
    "            data = transform(data)\n",
    "            print(f\"Applied {transform}: {data['image'].shape}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying transform {transform}: {e}\")\n",
    "        raise\n",
    "\n",
    "def handle_none(x, name=\"unknown\"):\n",
    "    if x is None:\n",
    "        print(f\"Warning: Encountered None {name}\")\n",
    "        return torch.zeros((1, 64, 64,96))\n",
    "    return x\n",
    "\n",
    "def print_shape(x, name=\"unknown\"):\n",
    "    if isinstance(x, dict):\n",
    "        print(f\"{name} Shape: {x['image'].shape if 'image' in x else 'No image'}\")\n",
    "    else:\n",
    "        print(f\"{name} Shape: {x.shape if hasattr(x, 'shape') else 'No shape'}\")\n",
    "    return x\n",
    "\n",
    "def validate_item(data):\n",
    "    if data is None:\n",
    "        print(\"Data is None\")\n",
    "        return False\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if value is None:\n",
    "                print(f\"{key} is None\")\n",
    "                return False\n",
    "            if isinstance(value, torch.Tensor) and value.numel() == 0:\n",
    "                print(f\"{key} is empty tensor\")\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def safe_transform(item):\n",
    "    try:\n",
    "        if not validate_item(item):\n",
    "            return None\n",
    "        return train_transforms(item)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing item: {item}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "class SafeDataset(CacheDataset):  \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        transformed = safe_transform(data)\n",
    "        if transformed is None:\n",
    "            \n",
    "            return self.__getitem__((index + 1) % len(self))\n",
    "        return transformed\n",
    "\n",
    "class ConvertToBinaryLabeld(MapTransform):\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        d['label'][d['label'] != 2] = 0\n",
    "        d['label'][d['label'] == 2] = 1\n",
    "        return d\n",
    "\n",
    "def get_dynamic_spatial_size(image_size, desired_size):\n",
    "    return tuple(min(img_size, desired_size) for img_size, desired_size in zip(image_size, desired_size))\n",
    "\n",
    "class DynamicRandCropByPosNegLabeld:\n",
    "    def __init__(self, transform, spatial_size):\n",
    "        self.transform = transform\n",
    "        self.spatial_size = spatial_size\n",
    "\n",
    "    def __call__(self, data):\n",
    "        image_shape = data['image'].shape[1:]\n",
    "        adaptive_spatial_size = get_dynamic_spatial_size(image_shape, self.spatial_size)\n",
    "        self.transform.spatial_size = adaptive_spatial_size\n",
    "        return self.transform(data)\n",
    "\n",
    "cropper = RandCropByPosNegLabeld(\n",
    "    keys=[\"image\", \"label\"],\n",
    "    label_key=\"label\",\n",
    "    spatial_size=(64, 64, 96),\n",
    "    pos=1,\n",
    "    neg=1,\n",
    "    num_samples=4,\n",
    "    image_key=\"image\",\n",
    "    image_threshold=0\n",
    ")\n",
    "\n",
    "dynamic_cropper = DynamicRandCropByPosNegLabeld(cropper, spatial_size)\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        Lambdad(keys=[\"image\", \"label\"], func=lambda x: print_shape(x, \"LoadImaged\")),\n",
    "        Lambdad(keys=[\"image\", \"label\"], func=lambda x: handle_none(x, \"LoadImaged\")),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Lambdad(keys=[\"image\", \"label\"], func=lambda x: print_shape(x, \"EnsureChannelFirstd\")),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        Lambdad(keys=[\"image\", \"label\"], func=lambda x: print_shape(x, \"Spacingd\")),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-175,\n",
    "            a_max=250,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        \n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\", allow_smaller=True),\n",
    "        Lambdad(keys=[\"image\", \"label\"], func=lambda x: print_shape(x, \"CropForegroundd\")),\n",
    "        dynamic_cropper,\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=(64, 64, 96)),\n",
    "        Resized(keys=[\"label\"], spatial_size=(64, 64, 96), mode='nearest'),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertToBinaryLabeld(keys=[\"label\"]),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"trilinear\", \"nearest\"),\n",
    "        ),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        ScaleIntensityRanged(keys=[\"image\"], a_min=-175, a_max=250, b_min=0.0, b_max=1.0, clip=True),\n",
    "        \n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"label\"),\n",
    "        Resized(keys=[\"image\", \"label\"], spatial_size=(64, 64, 96)),\n",
    "        Resized(keys=[\"label\"], spatial_size=(64, 64, 96), mode='nearest'),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        \n",
    "        ConvertToBinaryLabeld(keys=[\"label\"]),\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import SimpleITK as sitk\n",
    "from monai.transforms import Resize, ToTensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from monai.data import CacheDataset, load_decathlon_datalist\n",
    "\n",
    "\n",
    "# Define the data directory and the dataset JSON file\n",
    "data_dir = \"/home/systemx86/Pictures/Tumor_detection/Code_test/\"\n",
    "split_json = \"dataset_0.json\"\n",
    "datasets = data_dir + split_json\n",
    "\n",
    "# Load the training and validation data lists\n",
    "datalist = load_decathlon_datalist(datasets, True, \"training\")\n",
    "val_files = load_decathlon_datalist(datasets, True, \"validation\")\n",
    "\n",
    "# Create CacheDatasets for training and validation, applying transformations here\n",
    "train_ds = CacheDataset(\n",
    "    data=datalist,  \n",
    "    transform=train_transforms,\n",
    "    cache_num=24,\n",
    "    cache_rate=1.0,\n",
    "    num_workers=6,\n",
    ")\n",
    "\n",
    "val_ds = CacheDataset(\n",
    "    data=val_files,  \n",
    "    transform=val_transforms, \n",
    "    cache_num=6,\n",
    "    cache_rate=1.0,\n",
    "    num_workers=6,\n",
    ")\n",
    "\n",
    "# Wrap your dataset with the new class\n",
    "target_size = (64,64,96)  # Adjust spatial size according to memory constraints\n",
    "\n",
    "#train_ds = ConsistentSizeDataset(train_ds, target_size)\n",
    "#val_ds = ConsistentSizeDataset(val_ds, target_size)\n",
    "\n",
    "# Update DataLoader with the new dataset\n",
    "batch_size = 1  # Reduce batch size to lower memory usage\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=6, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=6, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "case_num = 8\n",
    "print(f\"Case number: {case_num}\")\n",
    "\n",
    "img = val_ds[case_num]['image']\n",
    "label = val_ds[case_num]['label']\n",
    "\n",
    "img_meta = val_ds[case_num]['image'].meta\n",
    "label_meta = val_ds[case_num]['label'].meta\n",
    "img_name = os.path.split(img_meta['filename_or_obj'])[1] \n",
    "\n",
    "# Print shapes\n",
    "img_shape = img.shape\n",
    "label_shape = label.shape\n",
    "print(f\"Image shape: {img_shape}, Label shape: {label_shape}\")\n",
    "\n",
    "\n",
    "image_path = f'/media/systemx86/A23089D03089AC3B/Users/arunk/Music/NIT_DATA/Task07_Pancreas/Task07_Pancreas/imagesTr/{img_name}'\n",
    "label_path = f'/media/systemx86/A23089D03089AC3B/Users/arunk/Music/NIT_DATA/Task07_Pancreas/Task07_Pancreas/labelsTr/{img_name}'\n",
    "\n",
    "raw_image_img = nib.load(image_path)\n",
    "raw_label_img = nib.load(label_path)\n",
    "\n",
    "raw_image_data = raw_image_img.get_fdata()\n",
    "raw_label_data = raw_label_img.get_fdata()\n",
    "\n",
    "\n",
    "slices = [30,40]\n",
    "\n",
    "fig, axes = plt.subplots(3, len(slices), figsize=(15, 15))\n",
    "\n",
    "for i, slice_idx in enumerate(slices):\n",
    "\n",
    "    axes[0, i].imshow(raw_image_data[:, :, slice_idx], cmap=\"gray\") \n",
    "    axes[0, i].set_title(f\"Raw Image - Slice {slice_idx}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    axes[1, i].imshow(raw_label_data[:, :, slice_idx], cmap=\"gray\")\n",
    "    axes[1, i].set_title(f\"Raw Label - Slice {slice_idx}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "\n",
    "    axes[2, i].imshow(label[0, :, :, slice_idx].squeeze().detach().cpu().numpy(), cmap=\"gray\")\n",
    "    axes[2, i].set_title(f\"Transformed Label - Slice {slice_idx}\")\n",
    "    axes[2, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "case_num = 8\n",
    "print(f\"Case number: {case_num}\")\n",
    "\n",
    "img = val_ds[case_num]['image']\n",
    "label = val_ds[case_num]['label']\n",
    "\n",
    "img_meta = val_ds[case_num]['image'].meta\n",
    "label_meta = val_ds[case_num]['label'].meta\n",
    "img_name = os.path.split(img_meta['filename_or_obj'])[1] \n",
    "\n",
    "img_shape = img.shape\n",
    "label_shape = label.shape\n",
    "print(f\"Image shape: {img_shape}, Label shape: {label_shape}\")\n",
    "\n",
    "\n",
    "label_path = f'/media/systemx86/A23089D03089AC3B/Users/arunk/Music/NIT_DATA/Task07_Pancreas/Task07_Pancreas/labelsTr/{img_name}'\n",
    "raw_label_img = nib.load(label_path)\n",
    "raw_label_data = raw_label_img.get_fdata()\n",
    "\n",
    "\n",
    "slices = [30, 50, 70]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(3, len(slices), figsize=(15, 15)) \n",
    "\n",
    "for i, slice_idx in enumerate(slices):\n",
    "   \n",
    "    axes[0, i].imshow(raw_label_data[:, :, slice_idx], cmap=\"gray\")\n",
    "    axes[0, i].set_title(f\"Raw Label - Slice {slice_idx}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    axes[1, i].imshow(label[0, :, :, slice_idx].squeeze().detach().cpu().numpy(), cmap=\"gray\")\n",
    "    axes[1, i].set_title(f\"Dataset Label - Slice {slice_idx}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "    img_slice = img[0, :, :, slice_idx].detach().cpu().numpy()  \n",
    "    img_slice = (img_slice - img_slice.min()) / (img_slice.max() - img_slice.min())  # Normalize image \n",
    "    axes[2, i].imshow(img_slice, cmap=\"gray\")\n",
    "    axes[2, i].contour(label[0, :, :, slice_idx].squeeze().detach().cpu().numpy(), colors='red', linewidths=0.5)\n",
    "    axes[2, i].set_title(f\"Image + Label - Slice {slice_idx}\")\n",
    "    axes[2, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler  # Added\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from monai.data import DataLoader, Dataset, decollate_batch\n",
    "from monai.networks.nets import UNETR  # Import UNETR\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "dice_val_best = -float('inf')\n",
    "best_dice=0.0\n",
    "global_step_best = 0\n",
    "patience = 20  \n",
    "epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "def validation(epoch_iterator_val, model, dice_metric, post_label, post_pred):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        post_pred = AsDiscrete(to_onehot=2, argmax=True)\n",
    "\n",
    "        for batch in epoch_iterator_val:\n",
    "            val_inputs, val_labels = batch.val_ds[\"image\"].to(device), batch.val_ds[\"label\"].to(device)\n",
    "            val_labels = val_ds.long()\n",
    "\n",
    "         \n",
    "\n",
    "            with autocast():\n",
    "                val_outputs = sliding_window_inference(val_inputs, (64,64,96), 4, model)\n",
    "\n",
    "            val_labels = torch.squeeze(val_labels, dim=1).long()\n",
    "            val_outputs = F.interpolate(val_outputs, size=val_labels.shape[1:], mode='trilinear', align_corners=False)\n",
    "\n",
    "            if torch.isnan(val_outputs).any() or torch.isinf(val_outputs).any():\n",
    "                print(\"NaN or Inf values detected in val_outputs during validation\")\n",
    "                break\n",
    "\n",
    "        \n",
    "            #print(\"val_labels shape before post_label:\", val_labels.shape) # Print shape before transformation\n",
    "            val_labels = post_label(val_labels)\n",
    "\n",
    "      \n",
    "            val_labels_list = decollate_batch(val_labels)\n",
    "            val_outputs_list = decollate_batch(val_outputs)\n",
    "\n",
    "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "            print(\"Predictions shape:\", val_outputs_list[0].shape)\n",
    "            print(\"Predictions data type:\", val_outputs_list[0].dtype)\n",
    "            print(\"Predictions min:\", val_outputs_list[0].min())\n",
    "            print(\"Predictions max:\", val_outputs_list[0].max())\n",
    "            print(\"Labels shape:\", val_labels_list[0].shape)\n",
    "            print(\"Labels data type:\", val_labels_list[0].dtype) \n",
    "            print(\"Labels min:\", val_labels_list[0].min())\n",
    "            print(\"Labels max:\", val_labels_list[0].max())\n",
    "\n",
    "            dice_metric(y_pred=val_output_convert, y=val_labels_list) \n",
    "\n",
    "        \n",
    "        mean_dice_val = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "        \n",
    "        return mean_dice_val\n",
    "    \n",
    "def train(global_step, train_loader, val_loader, model, optimizer, \n",
    "          loss_function, scaler, max_iterations, eval_num, root_dir, dice_metric, \n",
    "          post_label, post_pred,epochs_without_improvement):\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        try:\n",
    "            images, labels = batch.train_ds['image'].to(device), batch.train_ds['label'].to(device)\n",
    "            print(f\"Step: {step}\")\n",
    "            print(f\"Images type: {type(images)}, Labels type: {type(labels)}\")\n",
    "            \n",
    "            \n",
    "            print(f\"First image shape: {images[0].shape}, First label shape: {labels[0].shape}\")\n",
    "            \n",
    "       \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training at step {step}: {e}\")\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_step, val_batch in enumerate(val_loader):\n",
    "            val_images, val_labels = val_batch['image'], val_batch['label']\n",
    "            val_outputs = model(val_images)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "    epoch_loss = 0\n",
    "    accumulation_steps = 4 \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "    step = 0\n",
    "    \n",
    "    global global_step_best \n",
    "    \n",
    "   \n",
    "    global dice_val_best \n",
    "    global best_dice\n",
    "\n",
    "    for batch in epoch_iterator:\n",
    "        epoch_loss = 0.0\n",
    "        x = batch[\"image\"].to(device)\n",
    "        y = batch[\"label\"].to(device)\n",
    "        y = torch.squeeze(y, dim=1).long() \n",
    "        \n",
    "        \n",
    "        y = F.one_hot(y, num_classes=2).permute(0, 4, 1, 2, 3).float()\n",
    "\n",
    "        with autocast():\n",
    "            output = model(x) \n",
    "            logits = output \n",
    "            loss = loss_function(logits, y)  \n",
    "        \n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"NaN or Inf values detected in loss at global step {global_step}\")\n",
    "            print(\"Debugging NaNs/Infs:\")\n",
    "            print(f\"Inputs: {x}\")\n",
    "            print(f\"Labels: {y}\")\n",
    "            print(f\"Logits: {logits}\")\n",
    "            break\n",
    "\n",
    "        \n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "      \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_iterator.set_description(\"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, max_iterations, loss))\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "        if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
    "            epoch_iterator_val = tqdm(val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
    "            dice_val = validation(epoch_iterator_val, model, val_loader, dice_metric, post_label, post_pred)\n",
    "            epoch_loss /= (step + 1)\n",
    "            epoch_loss_values.append(epoch_loss)\n",
    "            metric_values.append(dice_val)\n",
    "\n",
    "            print(f\"Validation at step {global_step} - Dice Score: {dice_val}, Best Dice Score: {best_dice}\")\n",
    "\n",
    "            if dice_val > best_dice:\n",
    "                best_dice = dice_val\n",
    "                global_step_best = global_step\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
    "                print(\"Model Was Saved! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(best_dice, dice_val))\n",
    "                epochs_without_improvement = 0  # Reset counter if improvement\n",
    "            else:\n",
    "                print(\"Model Was Not Saved! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(best_dice, dice_val))\n",
    "                epochs_without_improvement += 1  # Increment counter if no improvement\n",
    "\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at step {global_step} - no improvement for {patience} epochs.\")\n",
    "                break  \n",
    "        step += 1\n",
    "\n",
    "    return global_step, global_step_best, epochs_without_improvement  \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = UNETR(\n",
    "    in_channels=1,\n",
    "    out_channels=2,  \n",
    "    img_size=(64,64,96),\n",
    "    feature_size=16,\n",
    "    hidden_size=768,\n",
    "    mlp_dim=3072,\n",
    "    num_heads=12,\n",
    "    proj_type=\"perceptron\",\n",
    "    norm_name=\"instance\",\n",
    "    res_block=True,\n",
    "    dropout_rate=0.0,\n",
    ").to(device)\n",
    "\n",
    " \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "loss_function = BCEWithLogitsLoss()\n",
    "scaler = GradScaler()\n",
    "\n",
    "max_iterations = 25000\n",
    "eval_num = 500\n",
    "post_label = AsDiscrete(to_onehot=2)  \n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=2)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "\n",
    "\n",
    "for _ in range(global_step, max_iterations):\n",
    "    try:\n",
    "        global_step, global_step_best, epochs_without_improvement = train(\n",
    "            global_step, train_loader, val_loader, model, optimizer, loss_function, scaler, \n",
    "            max_iterations, eval_num, root_dir, dice_metric, post_label, post_pred, \n",
    "            epochs_without_improvement  \n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training at global step {global_step}: {e}\")\n",
    "        break\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        break \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
